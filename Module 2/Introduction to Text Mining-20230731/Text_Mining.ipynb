{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZATION\n",
    "BREAK LONG SENTENCE INTO WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading gazetteers: <urlopen error [WinError 10054]\n",
      "[nltk_data]     An existing connection was forcibly closed by the\n",
      "[nltk_data]     remote host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gazetteers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gazetteers', 'gazetteers.zip', 'wordnet.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caprovinces.txt',\n",
       " 'countries.txt',\n",
       " 'isocountries.txt',\n",
       " 'mexstates.txt',\n",
       " 'nationalities.txt',\n",
       " 'uscities.txt',\n",
       " 'usstateabbrev.txt',\n",
       " 'usstates.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gazetteers.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Birmingham',\n",
       " 'Huntsville',\n",
       " 'Mobile',\n",
       " 'Montgomery',\n",
       " 'Anchorage',\n",
       " 'Chandler',\n",
       " 'Glendale',\n",
       " 'Mesa',\n",
       " 'Peoria',\n",
       " 'Phoenix',\n",
       " 'Scottsdale',\n",
       " 'Tempe',\n",
       " 'Tucson',\n",
       " 'Little Rock',\n",
       " 'Anaheim',\n",
       " 'Antioch',\n",
       " 'Bakersfield',\n",
       " 'Berkeley',\n",
       " 'Burbank',\n",
       " 'Chula Vista',\n",
       " 'Concord',\n",
       " 'Corona',\n",
       " 'Costa Mesa',\n",
       " 'Daly City',\n",
       " 'Downey',\n",
       " 'El Monte',\n",
       " 'Elk Grove',\n",
       " 'Escondido',\n",
       " 'Fairfield',\n",
       " 'Fontana',\n",
       " 'Fremont',\n",
       " 'Fresno',\n",
       " 'Fullerton',\n",
       " 'Garden Grove',\n",
       " 'Glendale',\n",
       " 'Hayward',\n",
       " 'Huntington Beach',\n",
       " 'Inglewood',\n",
       " 'Irvine',\n",
       " 'Lancaster',\n",
       " 'Long Beach',\n",
       " 'Los Angeles',\n",
       " 'Modesto',\n",
       " 'Moreno Valley',\n",
       " 'Oakland',\n",
       " 'Oceanside',\n",
       " 'Ontario',\n",
       " 'Orange',\n",
       " 'Oxnard',\n",
       " 'Palmdale',\n",
       " 'Pasadena',\n",
       " 'Pomona',\n",
       " 'Rancho Cucamonga',\n",
       " 'Richmond',\n",
       " 'Riverside',\n",
       " 'Roseville',\n",
       " 'Sacramento',\n",
       " 'Salinas',\n",
       " 'San Bernardino',\n",
       " 'San Buenaventura (Ventura)',\n",
       " 'San Diego',\n",
       " 'San Francisco',\n",
       " 'San Jose',\n",
       " 'Santa Ana',\n",
       " 'Santa Clara',\n",
       " 'Santa Clarita',\n",
       " 'Santa Rosa',\n",
       " 'Simi Valley',\n",
       " 'Stockton',\n",
       " 'Sunnyvale',\n",
       " 'Thousand Oaks',\n",
       " 'Torrance',\n",
       " 'Vallejo',\n",
       " 'Visalia',\n",
       " 'West Covina',\n",
       " 'Arvada',\n",
       " 'Aurora',\n",
       " 'Colorado Springs',\n",
       " 'Denver',\n",
       " 'Fort Collins',\n",
       " 'Lakewood',\n",
       " 'Pueblo',\n",
       " 'Thornton',\n",
       " 'Westminster',\n",
       " 'Bridgeport',\n",
       " 'Hartford',\n",
       " 'New Haven',\n",
       " 'Stamford',\n",
       " 'Waterbury',\n",
       " 'Washington',\n",
       " 'Cape Coral',\n",
       " 'Clearwater',\n",
       " 'Coral Springs',\n",
       " 'Fort Lauderdale',\n",
       " 'Gainesville',\n",
       " 'Hialeah',\n",
       " 'Hollywood',\n",
       " 'Jacksonville',\n",
       " 'Miami',\n",
       " 'Miramar',\n",
       " 'Orlando',\n",
       " 'Pembroke Pines',\n",
       " 'Pompano Beach',\n",
       " 'St. Petersburg',\n",
       " 'Tallahassee',\n",
       " 'Tampa',\n",
       " 'Atlanta',\n",
       " 'Columbus',\n",
       " 'Macon',\n",
       " 'Savannah',\n",
       " 'Honolulu',\n",
       " 'Boise',\n",
       " 'Aurora',\n",
       " 'Chicago',\n",
       " 'Joliet',\n",
       " 'Naperville',\n",
       " 'Peoria',\n",
       " 'Rockford',\n",
       " 'Springfield',\n",
       " 'Evansville',\n",
       " 'Fort Wayne',\n",
       " 'Indianapolis',\n",
       " 'South Bend',\n",
       " 'Cedar Rapids',\n",
       " 'Des Moines',\n",
       " 'Kansas City',\n",
       " 'Olathe',\n",
       " 'Overland Park',\n",
       " 'Topeka',\n",
       " 'Wichita',\n",
       " 'Lexington',\n",
       " 'Louisville-Jefferson County',\n",
       " 'Baton Rouge',\n",
       " 'Lafayette',\n",
       " 'New Orleans',\n",
       " 'Shreveport',\n",
       " 'Baltimore',\n",
       " 'Boston',\n",
       " 'Cambridge',\n",
       " 'Lowell',\n",
       " 'Springfield',\n",
       " 'Worcester',\n",
       " 'Ann Arbor',\n",
       " 'Detroit',\n",
       " 'Flint',\n",
       " 'Grand Rapids',\n",
       " 'Lansing',\n",
       " 'Sterling Heights',\n",
       " 'Warren',\n",
       " 'Minneapolis',\n",
       " 'St. Paul',\n",
       " 'Rochester',\n",
       " 'Jackson',\n",
       " 'Independence',\n",
       " 'Kansas City',\n",
       " 'Springfield',\n",
       " 'St. Louis',\n",
       " 'Lincoln',\n",
       " 'Omaha',\n",
       " 'Henderson',\n",
       " 'Las Vegas',\n",
       " 'North Las Vegas',\n",
       " 'Reno',\n",
       " 'Manchester',\n",
       " 'Elizabeth',\n",
       " 'Jersey City',\n",
       " 'Newark',\n",
       " 'Paterson',\n",
       " 'Albuquerque',\n",
       " 'Buffalo',\n",
       " 'New York City',\n",
       " 'Rochester',\n",
       " 'Syracuse',\n",
       " 'Yonkers',\n",
       " 'Cary',\n",
       " 'Charlotte',\n",
       " 'Durham',\n",
       " 'Fayetteville',\n",
       " 'Greensboro',\n",
       " 'High',\n",
       " 'Raleigh',\n",
       " 'Wilmington',\n",
       " 'Winston-Salem',\n",
       " 'Akron',\n",
       " 'Cincinnati',\n",
       " 'Cleveland',\n",
       " 'Columbus',\n",
       " 'Dayton',\n",
       " 'Toledo',\n",
       " 'Norman',\n",
       " 'Oklahoma City',\n",
       " 'Tulsa',\n",
       " 'Eugene',\n",
       " 'Portland',\n",
       " 'Salem',\n",
       " 'Allentown',\n",
       " 'Erie',\n",
       " 'Philadelphia',\n",
       " 'Pittsburgh',\n",
       " 'Providence',\n",
       " 'Charleston',\n",
       " 'Columbia',\n",
       " 'Sioux Falls',\n",
       " 'Chattanooga',\n",
       " 'Clarksville',\n",
       " 'Knoxville',\n",
       " 'Memphis',\n",
       " 'Nashville',\n",
       " 'Abilene',\n",
       " 'Amarillo',\n",
       " 'Arlington',\n",
       " 'Austin',\n",
       " 'Beaumont',\n",
       " 'Brownsville',\n",
       " 'Carrollton',\n",
       " 'Corpus Christi',\n",
       " 'Dallas',\n",
       " 'Denton',\n",
       " 'El Paso',\n",
       " 'Fort Worth',\n",
       " 'Garland',\n",
       " 'Grand Prairie',\n",
       " 'Houston',\n",
       " 'Irving',\n",
       " 'Killeen',\n",
       " 'Laredo',\n",
       " 'Lubbock',\n",
       " 'McAllen',\n",
       " 'Mesquite',\n",
       " 'Pasadena',\n",
       " 'Plano',\n",
       " 'San Antonio',\n",
       " 'Wichita Falls',\n",
       " 'Waco',\n",
       " 'Provo',\n",
       " 'Salt Lake City',\n",
       " 'West Valley City',\n",
       " 'Alexandria',\n",
       " 'Arlington',\n",
       " 'Chesapeake',\n",
       " 'Hampton',\n",
       " 'Newport News',\n",
       " 'Norfolk',\n",
       " 'Portsmouth',\n",
       " 'Richmond',\n",
       " 'Virginia Beach',\n",
       " 'Bellevue',\n",
       " 'Everett',\n",
       " 'Seattle',\n",
       " 'Spokane',\n",
       " 'Tacoma',\n",
       " 'Vancouver',\n",
       " 'Green Bay',\n",
       " 'Madison',\n",
       " 'Milwaukee']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = nltk.corpus.gazetteers.words(\"uscities.txt\")\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Birmingham Huntsville Mobile Montgomery Anchorage Chandler Glendale Mesa Peoria Phoenix Scottsdale Tempe Tucson Little Rock Anaheim Antioch Bakersfield Berkeley Burbank Chula Vista Concord Corona Costa Mesa Daly City Downey El Monte Elk Grove Escondido Fairfield Fontana Fremont Fresno Fullerton Garden Grove Glendale Hayward Huntington Beach Inglewood Irvine Lancaster Long Beach Los Angeles Modesto Moreno Valley Oakland Oceanside Ontario Orange Oxnard Palmdale Pasadena Pomona Rancho Cucamonga Richmond Riverside Roseville Sacramento Salinas San Bernardino San Buenaventura (Ventura) San Diego San Francisco San Jose Santa Ana Santa Clara Santa Clarita Santa Rosa Simi Valley Stockton Sunnyvale Thousand Oaks Torrance Vallejo Visalia West Covina Arvada Aurora Colorado Springs Denver Fort Collins Lakewood Pueblo Thornton Westminster Bridgeport Hartford New Haven Stamford Waterbury Washington Cape Coral Clearwater Coral Springs Fort Lauderdale Gainesville Hialeah Hollywood Jacksonville Miami Miramar Orlando Pembroke Pines Pompano Beach St. Petersburg Tallahassee Tampa Atlanta Columbus Macon Savannah Honolulu Boise Aurora Chicago Joliet Naperville Peoria Rockford Springfield Evansville Fort Wayne Indianapolis South Bend Cedar Rapids Des Moines Kansas City Olathe Overland Park Topeka Wichita Lexington Louisville-Jefferson County Baton Rouge Lafayette New Orleans Shreveport Baltimore Boston Cambridge Lowell Springfield Worcester Ann Arbor Detroit Flint Grand Rapids Lansing Sterling Heights Warren Minneapolis St. Paul Rochester Jackson Independence Kansas City Springfield St. Louis Lincoln Omaha Henderson Las Vegas North Las Vegas Reno Manchester Elizabeth Jersey City Newark Paterson Albuquerque Buffalo New York City Rochester Syracuse Yonkers Cary Charlotte Durham Fayetteville Greensboro High Raleigh Wilmington Winston-Salem Akron Cincinnati Cleveland Columbus Dayton Toledo Norman Oklahoma City Tulsa Eugene Portland Salem Allentown Erie Philadelphia Pittsburgh Providence Charleston Columbia Sioux Falls Chattanooga Clarksville Knoxville Memphis Nashville Abilene Amarillo Arlington Austin Beaumont Brownsville Carrollton Corpus Christi Dallas Denton El Paso Fort Worth Garland Grand Prairie Houston Irving Killeen Laredo Lubbock McAllen Mesquite Pasadena Plano San Antonio Wichita Falls Waco Provo Salt Lake City West Valley City Alexandria Arlington Chesapeake Hampton Newport News Norfolk Portsmouth Richmond Virginia Beach Bellevue Everett Seattle Spokane Tacoma Vancouver Green Bay Madison Milwaukee "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:500]:\n",
    "    print(word, sep= \" \", end= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = str(hamlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloaded because of an error\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Error: Resource punkt not found.\n",
    "#   Attempted to load tokenizers/punkt/english.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " \"'Birmingham\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Huntsville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Mobile\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Montgomery\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Anchorage\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Chandler\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Glendale\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Mesa\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Peoria\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Phoenix\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Scottsdale\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Tempe\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Tucson\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Little\",\n",
       " 'Rock',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Anaheim\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Antioch\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Bakersfield\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Berkeley\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Burbank\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Chula\",\n",
       " 'Vista',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Concord\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Corona\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Costa\",\n",
       " 'Mesa',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Daly\",\n",
       " 'City',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Downey\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'El\",\n",
       " 'Monte',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Elk\",\n",
       " 'Grove',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Escondido\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fairfield\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fontana\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fremont\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fresno\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fullerton\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Garden\",\n",
       " 'Grove',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Glendale\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Hayward\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Huntington\",\n",
       " 'Beach',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Inglewood\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Irvine\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Lancaster\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Long\",\n",
       " 'Beach',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Los\",\n",
       " 'Angeles',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Modesto\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Moreno\",\n",
       " 'Valley',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Oakland\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Oceanside\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Ontario\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Orange\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Oxnard\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Palmdale\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Pasadena\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Pomona\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Rancho\",\n",
       " 'Cucamonga',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Richmond\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Riverside\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Roseville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Sacramento\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Salinas\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'San\",\n",
       " 'Bernardino',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'San\",\n",
       " 'Buenaventura',\n",
       " '(',\n",
       " 'Ventura',\n",
       " ')',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'San\",\n",
       " 'Diego',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'San\",\n",
       " 'Francisco',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'San\",\n",
       " 'Jose',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Santa\",\n",
       " 'Ana',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Santa\",\n",
       " 'Clara',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Santa\",\n",
       " 'Clarita',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Santa\",\n",
       " 'Rosa',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Simi\",\n",
       " 'Valley',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Stockton\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Sunnyvale\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Thousand\",\n",
       " 'Oaks',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Torrance\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Vallejo\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Visalia\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'West\",\n",
       " 'Covina',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Arvada\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Aurora\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Colorado\",\n",
       " 'Springs',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Denver\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fort\",\n",
       " 'Collins',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Lakewood\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Pueblo\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Thornton\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Westminster\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Bridgeport\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Hartford\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'New\",\n",
       " 'Haven',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Stamford\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Waterbury\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Washington\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Cape\",\n",
       " 'Coral',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Clearwater\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Coral\",\n",
       " 'Springs',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fort\",\n",
       " 'Lauderdale',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Gainesville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Hialeah\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Hollywood\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Jacksonville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Miami\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Miramar\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Orlando\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Pembroke\",\n",
       " 'Pines',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Pompano\",\n",
       " 'Beach',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'St\",\n",
       " '.',\n",
       " 'Petersburg',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Tallahassee\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Tampa\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Atlanta\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Columbus\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Macon\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Savannah\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Honolulu\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Boise\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Aurora\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Chicago\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Joliet\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Naperville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Peoria\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Rockford\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Springfield\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Evansville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fort\",\n",
       " 'Wayne',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Indianapolis\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'South\",\n",
       " 'Bend',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Cedar\",\n",
       " 'Rapids',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Des\",\n",
       " 'Moines',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Kansas\",\n",
       " 'City',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Olathe\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Overland\",\n",
       " 'Park',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Topeka\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Wichita\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Lexington\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Louisville-Jefferson\",\n",
       " 'County',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Baton\",\n",
       " 'Rouge',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Lafayette\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'New\",\n",
       " 'Orleans',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Shreveport\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Baltimore\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Boston\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Cambridge\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Lowell\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Springfield\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Worcester\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Ann\",\n",
       " 'Arbor',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Detroit\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Flint\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Grand\",\n",
       " 'Rapids',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Lansing\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Sterling\",\n",
       " 'Heights',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Warren\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Minneapolis\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'St\",\n",
       " '.',\n",
       " 'Paul',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Rochester\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Jackson\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Independence\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Kansas\",\n",
       " 'City',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Springfield\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'St\",\n",
       " '.',\n",
       " 'Louis',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Lincoln\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Omaha\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Henderson\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Las\",\n",
       " 'Vegas',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'North\",\n",
       " 'Las',\n",
       " 'Vegas',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Reno\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Manchester\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Elizabeth\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Jersey\",\n",
       " 'City',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Newark\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Paterson\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Albuquerque\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Buffalo\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'New\",\n",
       " 'York',\n",
       " 'City',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Rochester\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Syracuse\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Yonkers\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Cary\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Charlotte\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Durham\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fayetteville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Greensboro\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'High\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Raleigh\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Wilmington\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Winston-Salem\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Akron\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Cincinnati\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Cleveland\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Columbus\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Dayton\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Toledo\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Norman\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Oklahoma\",\n",
       " 'City',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Tulsa\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Eugene\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Portland\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Salem\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Allentown\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Erie\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Philadelphia\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Pittsburgh\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Providence\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Charleston\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Columbia\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Sioux\",\n",
       " 'Falls',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Chattanooga\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Clarksville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Knoxville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Memphis\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Nashville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Abilene\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Amarillo\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Arlington\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Austin\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Beaumont\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Brownsville\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Carrollton\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Corpus\",\n",
       " 'Christi',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Dallas\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Denton\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'El\",\n",
       " 'Paso',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Fort\",\n",
       " 'Worth',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Garland\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Grand\",\n",
       " 'Prairie',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Houston\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Irving\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Killeen\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Laredo\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Lubbock\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'McAllen\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Mesquite\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Pasadena\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Plano\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'San\",\n",
       " 'Antonio',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Wichita\",\n",
       " 'Falls',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Waco\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Provo\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Salt\",\n",
       " 'Lake',\n",
       " 'City',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'West\",\n",
       " 'Valley',\n",
       " 'City',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Alexandria\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Arlington\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Chesapeake\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Hampton\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Newport\",\n",
       " 'News',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Norfolk\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Portsmouth\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Richmond\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Virginia\",\n",
       " 'Beach',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Bellevue\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Everett\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Seattle\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Spokane\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Tacoma\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Vancouver\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Green\",\n",
       " 'Bay',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Madison\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Milwaukee\",\n",
       " \"'\",\n",
       " ']']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet_tokens = word_tokenize(hamlet)\n",
    "hamlet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'N', 'Nr', '_N', '__add__', '__and__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__init__', '__init_subclass__', '__ior__', '__isub__', '__iter__', '__le__', '__len__', '__lt__', '__missing__', '__module__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__weakref__', '_cumulative_frequencies', '_keep_positive', 'clear', 'copy', 'elements', 'freq', 'fromkeys', 'get', 'hapaxes', 'items', 'keys', 'max', 'most_common', 'pformat', 'plot', 'pop', 'popitem', 'pprint', 'r_Nr', 'setdefault', 'subtract', 'tabulate', 'total', 'update', 'values']\n"
     ]
    }
   ],
   "source": [
    "print(dir(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'best',\n",
       " 'and',\n",
       " 'most',\n",
       " 'beautiful',\n",
       " 'things',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'or',\n",
       " 'even',\n",
       " 'touched',\n",
       " ',',\n",
       " 'they',\n",
       " 'mus',\n",
       " 'be',\n",
       " 'felt',\n",
       " 'wiht',\n",
       " 'th',\n",
       " 'eheart']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"The best and most beautiful things in the world cannot be seen or even touched, they mus be felt wiht th eheart\"\n",
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best'),\n",
       " ('best', 'and'),\n",
       " ('and', 'most'),\n",
       " ('most', 'beautiful'),\n",
       " ('beautiful', 'things'),\n",
       " ('things', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'world'),\n",
       " ('world', 'can'),\n",
       " ('can', 'not'),\n",
       " ('not', 'be'),\n",
       " ('be', 'seen'),\n",
       " ('seen', 'or'),\n",
       " ('or', 'even'),\n",
       " ('even', 'touched'),\n",
       " ('touched', ','),\n",
       " (',', 'they'),\n",
       " ('they', 'mus'),\n",
       " ('mus', 'be'),\n",
       " ('be', 'felt'),\n",
       " ('felt', 'wiht'),\n",
       " ('wiht', 'th'),\n",
       " ('th', 'eheart')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and'),\n",
       " ('best', 'and', 'most'),\n",
       " ('and', 'most', 'beautiful'),\n",
       " ('most', 'beautiful', 'things'),\n",
       " ('beautiful', 'things', 'in'),\n",
       " ('things', 'in', 'the'),\n",
       " ('in', 'the', 'world'),\n",
       " ('the', 'world', 'can'),\n",
       " ('world', 'can', 'not'),\n",
       " ('can', 'not', 'be'),\n",
       " ('not', 'be', 'seen'),\n",
       " ('be', 'seen', 'or'),\n",
       " ('seen', 'or', 'even'),\n",
       " ('or', 'even', 'touched'),\n",
       " ('even', 'touched', ','),\n",
       " ('touched', ',', 'they'),\n",
       " (',', 'they', 'mus'),\n",
       " ('they', 'mus', 'be'),\n",
       " ('mus', 'be', 'felt'),\n",
       " ('be', 'felt', 'wiht'),\n",
       " ('felt', 'wiht', 'th'),\n",
       " ('wiht', 'th', 'eheart')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and', 'most'),\n",
       " ('best', 'and', 'most', 'beautiful'),\n",
       " ('and', 'most', 'beautiful', 'things'),\n",
       " ('most', 'beautiful', 'things', 'in'),\n",
       " ('beautiful', 'things', 'in', 'the'),\n",
       " ('things', 'in', 'the', 'world'),\n",
       " ('in', 'the', 'world', 'can'),\n",
       " ('the', 'world', 'can', 'not'),\n",
       " ('world', 'can', 'not', 'be'),\n",
       " ('can', 'not', 'be', 'seen'),\n",
       " ('not', 'be', 'seen', 'or'),\n",
       " ('be', 'seen', 'or', 'even'),\n",
       " ('seen', 'or', 'even', 'touched'),\n",
       " ('or', 'even', 'touched', ','),\n",
       " ('even', 'touched', ',', 'they'),\n",
       " ('touched', ',', 'they', 'mus'),\n",
       " (',', 'they', 'mus', 'be'),\n",
       " ('they', 'mus', 'be', 'felt'),\n",
       " ('mus', 'be', 'felt', 'wiht'),\n",
       " ('be', 'felt', 'wiht', 'th'),\n",
       " ('felt', 'wiht', 'th', 'eheart')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams = list(nltk.ngrams(quotes_tokens,4 ))\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING \n",
    "NORMALIZE WORDS INTO THEIR ROOT FORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem = [\"give\",\"giving\", \"given\",\"gave\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for word in words_to_stem:\n",
    "    print(word+ \":\" +pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "giving:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "for word in words_to_stem:\n",
    "    print(word+ \":\" +lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEMMATIZATION\n",
    "GROUPS TOGETHER DIFFERENT FORMS OF WORD CALLED LEMMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_len = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading omw-1.4: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading due to error\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#  Error: Resource omw-1.4 not found.\n",
    "#   Attempted to load corpora/omw-1.4.zip/omw-1.4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_len\u001b[39m.\u001b[39;49mlemmatize(\u001b[39m\"\u001b[39;49m\u001b[39mcorpora\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n\u001b[0;32m     91\u001b[0m \u001b[39m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[0;32m   1175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49momw_prov()\n\u001b[0;32m   1178\u001b[0m \u001b[39m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang_data \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m provdict \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1284\u001b[0m provdict[\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1285\u001b[0m fileids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_omw_reader\u001b[39m.\u001b[39;49mfileids()\n\u001b[0;32m   1286\u001b[0m \u001b[39mfor\u001b[39;00m fileid \u001b[39min\u001b[39;00m fileids:\n\u001b[0;32m   1287\u001b[0m     prov, langfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(fileid)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "word_len.lemmatize(\"corpora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m words \u001b[39min\u001b[39;00m words_to_stem:\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mprint\u001b[39m(words\u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m word_len\u001b[39m.\u001b[39;49mlemmatize(words))\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n\u001b[0;32m     91\u001b[0m \u001b[39m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[0;32m   1175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49momw_prov()\n\u001b[0;32m   1178\u001b[0m \u001b[39m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang_data \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m provdict \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1284\u001b[0m provdict[\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1285\u001b[0m fileids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_omw_reader\u001b[39m.\u001b[39;49mfileids()\n\u001b[0;32m   1286\u001b[0m \u001b[39mfor\u001b[39;00m fileid \u001b[39min\u001b[39;00m fileids:\n\u001b[0;32m   1287\u001b[0m     prov, langfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(fileid)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+ \":\" + word_len.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10054]\n",
      "[nltk_data]     An existing connection was forcibly closed by the\n",
      "[nltk_data]     remote host>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "#stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mlen\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Msc 2\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Msc 2/nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Msc 2\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Msc 2\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'[-.?!,:;()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hey world, this is just a trial. Learning text mining using python. 1234567890. | Dont give up, Give in\"\n",
    "post_punctuation = []\n",
    "for words in sentence:\n",
    "    word = punctuation.sub(\"\", words)\n",
    "    if len(word)>0:\n",
    "        post_punctuation.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(post_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"This sentence will be used to display the parts of speech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_tokens = word_tokenize(sen)\n",
    "sen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sen_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAMED ENTITY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_sentence = \"India is a big country. TAJ MAHAL is in India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_tokens = word_tokenize(NE_sentence)\n",
    "NE_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in NE_tokens:\n",
    "    print(nltk.pos_tag([words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_tags = nltk.pos_tag(NE_tokens)\n",
    "NE_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = ne_chunk(NE_tags)\n",
    "print(NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHUNKING\n",
    "PICKING UP INDIVIDUAL PIECES OF INFORMATION AND GROUPING THEM INTO BIGGER PIECES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"This lion ate deer who was eating fresh grass\"\n",
    "line_tokens = nltk.pos_tag(word_tokenize(line))\n",
    "line_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np = r\"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammar_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_result = chunk_parser.parse(line_tokens)\n",
    "print(chunk_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE \n",
    "#MOVIE REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(movie_reviews.fileids(\"pos\")))\n",
    "print(\" \")\n",
    "print(movie_reviews.fileids(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_reviews.fileids(\"pos\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_reviews.fileids(\"neg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rev = movie_reviews.fileids(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = nltk.corpus.movie_reviews.words(\"pos/cv143_19666.txt\")\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the tokens into string in order to use counvectorizer\n",
    "rev_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rev in pos_rev:\n",
    "    rev_text_pos = nltk.corpus.movie_reviews.words(rev)\n",
    "    review_one_string = \" \".join(rev_text_pos)  \n",
    "    review_one_string = review_one_string.replace(' ,', ',')\n",
    "    review_one_string = review_one_string.replace(' .' , '.')\n",
    "    review_one_string = review_one_string.replace(\"\\' \" , \"'\")\n",
    "    review_one_string = review_one_string.replace(\" \\'\" , \"'\")\n",
    "    rev_list.append(review_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev = movie_reviews.fileids(\"neg\")\n",
    "for rev in neg_rev:\n",
    "    rev_text_neg = nltk.corpus.movie_reviews.words(rev)\n",
    "    review_one_string = \" \".join(rev_text_neg)  \n",
    "    review_one_string = review_one_string.replace(\" ,\", \",\")\n",
    "    review_one_string = review_one_string.replace(\" .\" , \".\")\n",
    "    review_one_string = review_one_string.replace(\"\\' \" , \"'\")\n",
    "    review_one_string = review_one_string.replace(\" \\'\", \"'\")\n",
    "    rev_list.append(review_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_targets = np.zeros((1000,), dtype = np.int)\n",
    "pos_targets = np.ones((1000,), dtype = np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "for neg_tar in neg_targets:\n",
    "    target_list.append(neg_tar)\n",
    "for pos_tar in pos_targets:\n",
    "    target_list.append(pos_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(lowercase= True, stop_words = \"english\", min_df = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count = count.fit_transform(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names = count.get_feature_names()\n",
    "x_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vect = pd.DataFrame(x_count.toarray(), columns = x_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_vect, y, test_size = 0.25, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred_gnb = gnb.fit(x_train, y_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cv = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_cv.predict(x_test)\n",
    "type(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score  =confusion_matrix(y_test, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
